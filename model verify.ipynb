{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e63acb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import d2lzh_pytorch as d2l\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e6d2a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySequential(\n",
      "  (0): Conv3d(3, 16, kernel_size=(5, 7, 7), stride=(1, 1, 1))\n",
      "  (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): Sigmoid()\n",
      "  (3): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (4): MyFlattenLayer()\n",
      "  (5): Linear(in_features=10000, out_features=480, bias=True)\n",
      "  (6): BatchNorm1d(13, eps=480, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (7): Sigmoid()\n",
      "  (8): Linear(in_features=480, out_features=336, bias=True)\n",
      "  (9): BatchNorm1d(13, eps=336, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (10): Sigmoid()\n",
      "  (11): LSTM(336, 1024, batch_first=True)\n",
      "  (12): Linear(in_features=1024, out_features=64, bias=True)\n",
      "  (13): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (14): Sigmoid()\n",
      "  (15): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    from collections import OrderedDict\n",
    "    def __init__(self, *args):\n",
    "        super(MySequential, self).__init__()\n",
    "        if len(args) == 1 and isinstance(args[0], OrderedDict): # 如果传入的是一个OrderedDict\n",
    "            for key, module in args[0].items():\n",
    "                self.add_module(key, module)  # add_module方法会将module添加进self._modules(一个OrderedDict)\n",
    "        else:  # 传入的是一些Module\n",
    "            for idx, module in enumerate(args):\n",
    "                self.add_module(str(idx), module)\n",
    "    def forward(self, input):\n",
    "        # self._modules返回一个 OrderedDict，保证会按照成员添加时的顺序遍历成\n",
    "        for module in self._modules.values():\n",
    "            if type(module) is torch.nn.modules.rnn.LSTM:\n",
    "#                 input = input.view(-1, 30, 84*4)\n",
    "                input, (h_n, c_n) = module(input)\n",
    "                input = input[:, -1, :]\n",
    "                print('lstm', input.size())\n",
    "            else:\n",
    "                input = module(input)\n",
    "                print('other', input.size())\n",
    "        return input\n",
    "\n",
    "net = MySequential(\n",
    "            nn.Conv3d(3, 16, (5, 7, 7), stride=1, padding=0), # in_channels, out_channels, kernel_size\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool3d(2, 2), # kernel_size, stride\n",
    "#             nn.Conv3d(16, 64, 5),\n",
    "# #             nn.BatchNorm2d(64),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.MaxPool3d(2, 2),\n",
    "            d2l.MyFlattenLayer(),\n",
    "            nn.Linear(16*25*25, 480),\n",
    "            nn.BatchNorm1d(13, 480),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(480, 84*4),\n",
    "            nn.BatchNorm1d(13, 84*4),\n",
    "            nn.Sigmoid(),\n",
    "            #nn.Linear(84, 10)\n",
    "            nn.LSTM(84*4, 1024, num_layers=1, batch_first=True),\n",
    "            nn.Linear(1024, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "print(net)\n",
    "\n",
    "# X = torch.rand(10, 1, 57, 57)\n",
    "# print(net(X))\n",
    "# Y = net(X)\n",
    "# print(Y.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a22aa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2517])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "global X, Y\n",
    "X = torch.load(\"./X.pt\").float()\n",
    "Y = torch.load(\"./Y.pt\").float()\n",
    "X = X.view(2516, 3, 57, 57)\n",
    "print(Y.size())\n",
    "# train_loader = DataLoader(X, batch_size=128, shuffle=False)\n",
    "# test_loader = DataLoader(Y, batch_size=128, shuffle=False)\n",
    "\n",
    "def data_iter_random(X, Y, batch_size, num_steps, device=None):\n",
    "#     print(X.size())\n",
    "    num_examples = (len(Y) - num_steps)\n",
    "#     print('examples', num_examples)\n",
    "    epoch_size = num_examples // batch_size\n",
    "#     print('epoch', epoch_size)\n",
    "    example_indices = list(range(num_examples))\n",
    "#     print(example_indices)\n",
    "#     random.shuffle(example_indices)\n",
    "    \n",
    "    def _data(pos, data):\n",
    "        if data is X:\n",
    "#             print(pos, pos+num_steps)\n",
    "#             print(data[pos:pos + num_steps, :, :, :].size())\n",
    "            return data[pos:pos + num_steps, :, :, :]\n",
    "        if data is Y:\n",
    "#             print(pos)\n",
    "            return data[pos + num_steps]\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    for i in range(epoch_size):\n",
    "        # 每次读取batch_size个随机样本\n",
    "        i = i * batch_size\n",
    "        batch_indices = example_indices[i: i + batch_size]\n",
    "#         print(batch_indices)\n",
    "        XX = [_data(j, X) for j in batch_indices]\n",
    "        YY = [_data(j, Y) for j in batch_indices]\n",
    "        XX = torch.stack(XX)\n",
    "        YY = torch.stack(YY)\n",
    "        XX = XX.transpose(1, 2)\n",
    "        yield XX, YY\n",
    "        \n",
    "# for xx, yy in data_iter_random(X, Y, 2, 2):\n",
    "#     print('X: ', xx[0].size(), '\\nY:', yy, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "198362cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2516, 3, 57, 57])\n",
      "other torch.Size([1, 16, 26, 51, 51])\n",
      "other torch.Size([1, 16, 26, 51, 51])\n",
      "other torch.Size([1, 16, 26, 51, 51])\n",
      "other torch.Size([1, 16, 13, 25, 25])\n",
      "other torch.Size([1, 13, 10000])\n",
      "other torch.Size([1, 13, 480])\n",
      "other torch.Size([1, 13, 480])\n",
      "other torch.Size([1, 13, 480])\n",
      "other torch.Size([1, 13, 336])\n",
      "other torch.Size([1, 13, 336])\n",
      "other torch.Size([1, 13, 336])\n",
      "lstm torch.Size([1, 1024])\n",
      "other torch.Size([1, 64])\n",
      "other torch.Size([1, 64])\n",
      "other torch.Size([1, 64])\n",
      "other torch.Size([1, 1])\n",
      "tensor([[1.6774]], grad_fn=<ViewBackward0>)\n",
      "other torch.Size([1, 16, 26, 51, 51])\n",
      "other torch.Size([1, 16, 26, 51, 51])\n",
      "other torch.Size([1, 16, 26, 51, 51])\n",
      "other torch.Size([1, 16, 13, 25, 25])\n",
      "other torch.Size([1, 13, 10000])\n",
      "other torch.Size([1, 13, 480])\n",
      "other torch.Size([1, 13, 480])\n",
      "other torch.Size([1, 13, 480])\n",
      "other torch.Size([1, 13, 336])\n",
      "other torch.Size([1, 13, 336])\n",
      "other torch.Size([1, 13, 336])\n",
      "lstm torch.Size([1, 1024])\n",
      "other torch.Size([1, 64])\n",
      "other torch.Size([1, 64])\n",
      "other torch.Size([1, 64])\n",
      "other torch.Size([1, 1])\n",
      "tensor([[1.6780]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 1])\n",
      "l [4246.3525390625, 2120.93798828125]\n",
      "e 1\n"
     ]
    }
   ],
   "source": [
    "# net = MySequential()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "checkpoint = torch.load('./model1.pt')\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "l = checkpoint['loss']\n",
    "e = checkpoint['epoch']\n",
    "net.eval()\n",
    "\n",
    "\n",
    "print(X.size())\n",
    "x_test1 = X[0:30, :, :, :]\n",
    "x_test2 = X[1400:1430, :, :, :]\n",
    "x_test2 = torch.rand(30, 3, 57, 57)\n",
    "# print(x_test1[:, 1, :, :])\n",
    "x_test1 = x_test1.transpose(0, 1)\n",
    "x_test1 = x_test1.view(1, 3, 30, 57, 57)\n",
    "y_test1 = net(x_test1)\n",
    "y_test1 = y_test1.view(y_test.shape[0], -1)\n",
    "print(y_test1)\n",
    "x_test2 = x_test2.transpose(0, 1)\n",
    "x_test2 = x_test2.view(1, 3, 30, 57, 57)\n",
    "y_test2 = net(x_test2)\n",
    "y_test2 = y_test2.view(y_test.shape[0], -1)\n",
    "print(y_test2)\n",
    "print(y_test2.size())\n",
    "\n",
    "print('l', l)\n",
    "print('e', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebd282c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
