{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c606dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import d2lzh_pytorch as d2l\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5c8992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    from collections import OrderedDict\n",
    "    def __init__(self, *args):\n",
    "        super(MySequential, self).__init__()\n",
    "        if len(args) == 1 and isinstance(args[0], OrderedDict): # 如果传入的是一个OrderedDict\n",
    "            for key, module in args[0].items():\n",
    "                self.add_module(key, module)  # add_module方法会将module添加进self._modules(一个OrderedDict)\n",
    "        else:  # 传入的是一些Module\n",
    "            for idx, module in enumerate(args):\n",
    "                self.add_module(str(idx), module)\n",
    "    def forward(self, input):\n",
    "        # self._modules返回一个 OrderedDict，保证会按照成员添加时的顺序遍历成\n",
    "        for module in self._modules.values():\n",
    "            if type(module) is torch.nn.modules.rnn.LSTM:\n",
    "#                 input = input.view(-1, 30, 84*4)\n",
    "                input, (h_n, c_n) = module(input)\n",
    "                input = input[:, -1, :]\n",
    "#                 print('lstm', input.size())\n",
    "            else:\n",
    "                input = module(input)\n",
    "#                 print('other', input.size())\n",
    "        return input\n",
    "\n",
    "net = MySequential(\n",
    "            nn.Conv3d(3, 16, (5, 7, 7), stride=1, padding=0), # in_channels, out_channels, kernel_size\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool3d(2, 2), # kernel_size, stride\n",
    "#             nn.Conv3d(16, 64, 5),\n",
    "# #             nn.BatchNorm2d(64),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.MaxPool3d(2, 2),\n",
    "            d2l.MyFlattenLayer(),\n",
    "            nn.Linear(16*25*25, 480),\n",
    "            nn.BatchNorm1d(13, 480),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(480, 84*4),\n",
    "            nn.BatchNorm1d(13, 84*4),\n",
    "            nn.Sigmoid(),\n",
    "            #nn.Linear(84, 10)\n",
    "            nn.LSTM(84*4, 1024, num_layers=1, batch_first=True),\n",
    "            nn.Linear(1024, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "print(net)\n",
    "\n",
    "# X = torch.rand(10, 1, 57, 57)\n",
    "# print(net(X))\n",
    "# Y = net(X)\n",
    "# print(Y.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bfe6c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch5(net, batch_size, optimizer, device, num_epochs):\n",
    "    global X, Y\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    loss = torch.nn.MSELoss()\n",
    "    batch_count = 0\n",
    "    L = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, start = 0.0, time.time()\n",
    "        train_iter = data_iter_random(X[:200, :, :, :], Y[:200], 128, 30)\n",
    "        for x, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(x)\n",
    "            y_hat = y_hat.view(y_hat.shape[0])\n",
    "            print(y.size())\n",
    "            print(y_hat.size())\n",
    "            \n",
    "            l = loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            batch_count += 1\n",
    "            L.append(train_l_sum / batch_count)\n",
    "        print('epoch %d, loss %.4f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, time.time() - start))\n",
    "#         x_test = X[1200:1230, :, :, :]\n",
    "#         x_test = x_test.transpose(0, 1)\n",
    "#         x_test = x_test.view(1, 3, 30, 57, 57)\n",
    "#         print(net(x_test))\n",
    "#         x_test = X[0:30, :, :, :]\n",
    "#         x_test = x_test.transpose(0, 1)\n",
    "#         x_test = x_test.view(1, 3, 30, 57, 57)\n",
    "#         print(net(x_test))\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'model_state_dict': net.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': L},\n",
    "                   './model1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87207b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2517])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "global X, Y\n",
    "X = torch.load(\"./X.pt\").float()\n",
    "Y = torch.load(\"./Y.pt\").float()\n",
    "X = X.view(2516, 3, 57, 57)\n",
    "print(Y.size())\n",
    "# train_loader = DataLoader(X, batch_size=128, shuffle=False)\n",
    "# test_loader = DataLoader(Y, batch_size=128, shuffle=False)\n",
    "\n",
    "def data_iter_random(X, Y, batch_size, num_steps, device=None):\n",
    "#     print(X.size())\n",
    "    num_examples = (len(Y) - num_steps)\n",
    "#     print('examples', num_examples)\n",
    "    epoch_size = num_examples // batch_size\n",
    "#     print('epoch', epoch_size)\n",
    "    example_indices = list(range(num_examples))\n",
    "#     print(example_indices)\n",
    "#     random.shuffle(example_indices)\n",
    "    \n",
    "    def _data(pos, data):\n",
    "        if data is X:\n",
    "#             print(pos, pos+num_steps)\n",
    "#             print(data[pos:pos + num_steps, :, :, :].size())\n",
    "            return data[pos:pos + num_steps, :, :, :]\n",
    "        if data is Y:\n",
    "#             print(pos)\n",
    "            return data[pos + num_steps]\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    for i in range(epoch_size):\n",
    "        # 每次读取batch_size个随机样本\n",
    "        i = i * batch_size\n",
    "        batch_indices = example_indices[i: i + batch_size]\n",
    "#         print(batch_indices)\n",
    "        XX = [_data(j, X) for j in batch_indices]\n",
    "        YY = [_data(j, Y) for j in batch_indices]\n",
    "        XX = torch.stack(XX)\n",
    "        YY = torch.stack(YY)\n",
    "        XX = XX.transpose(1, 2)\n",
    "        yield XX, YY\n",
    "        \n",
    "# for xx, yy in data_iter_random(X, Y, 2, 2):\n",
    "#     print('X: ', xx[0].size(), '\\nY:', yy, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb7a93c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cpu\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 13, 25, 25])\n",
      "other torch.Size([128, 13, 10000])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 336])\n",
      "other torch.Size([128, 13, 336])\n",
      "other torch.Size([128, 13, 336])\n",
      "lstm torch.Size([128, 1024])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 1])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "epoch 1, loss 4246.3525, time 110.5 sec\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 13, 25, 25])\n",
      "other torch.Size([128, 13, 10000])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 336])\n",
      "other torch.Size([128, 13, 336])\n",
      "other torch.Size([128, 13, 336])\n",
      "lstm torch.Size([128, 1024])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 1])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "epoch 2, loss 2120.9380, time 106.7 sec\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 13, 25, 25])\n",
      "other torch.Size([128, 13, 10000])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 336])\n",
      "other torch.Size([128, 13, 336])\n",
      "other torch.Size([128, 13, 336])\n",
      "lstm torch.Size([128, 1024])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 1])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "epoch 3, loss 1412.4373, time 104.3 sec\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 13, 25, 25])\n",
      "other torch.Size([128, 13, 10000])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 336])\n",
      "other torch.Size([128, 13, 336])\n",
      "other torch.Size([128, 13, 336])\n",
      "lstm torch.Size([128, 1024])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 1])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "epoch 4, loss 1058.2529, time 99.6 sec\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 13, 25, 25])\n",
      "other torch.Size([128, 13, 10000])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 336])\n",
      "other torch.Size([128, 13, 336])\n",
      "other torch.Size([128, 13, 336])\n",
      "lstm torch.Size([128, 1024])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 1])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "epoch 5, loss 845.7151, time 93.9 sec\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 13, 25, 25])\n",
      "other torch.Size([128, 13, 10000])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 336])\n",
      "other torch.Size([128, 13, 336])\n",
      "other torch.Size([128, 13, 336])\n",
      "lstm torch.Size([128, 1024])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 1])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "epoch 6, loss 704.0216, time 93.5 sec\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 13, 25, 25])\n",
      "other torch.Size([128, 13, 10000])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 336])\n",
      "other torch.Size([128, 13, 336])\n",
      "other torch.Size([128, 13, 336])\n",
      "lstm torch.Size([128, 1024])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 1])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "epoch 7, loss 602.8087, time 96.3 sec\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 13, 25, 25])\n",
      "other torch.Size([128, 13, 10000])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 336])\n",
      "other torch.Size([128, 13, 336])\n",
      "other torch.Size([128, 13, 336])\n",
      "lstm torch.Size([128, 1024])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 1])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "epoch 8, loss 526.8953, time 98.3 sec\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 13, 25, 25])\n",
      "other torch.Size([128, 13, 10000])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 336])\n",
      "other torch.Size([128, 13, 336])\n",
      "other torch.Size([128, 13, 336])\n",
      "lstm torch.Size([128, 1024])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 1])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "epoch 9, loss 467.8423, time 103.4 sec\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 26, 51, 51])\n",
      "other torch.Size([128, 16, 13, 25, 25])\n",
      "other torch.Size([128, 13, 10000])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 480])\n",
      "other torch.Size([128, 13, 336])\n",
      "other torch.Size([128, 13, 336])\n",
      "other torch.Size([128, 13, 336])\n",
      "lstm torch.Size([128, 1024])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 64])\n",
      "other torch.Size([128, 1])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "epoch 10, loss 420.5813, time 100.8 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain_ch5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36mtrain_ch5\u001b[0;34m(net, batch_size, optimizer, device, num_epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 14\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m y_hat\u001b[38;5;241m.\u001b[39mview(y_hat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36mMySequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mother\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/conv.py:592\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/conv.py:587\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    577\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    578\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    586\u001b[0m     )\n\u001b[0;32m--> 587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 500\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "batch_size = 128\n",
    "device = 'cpu'\n",
    "train_ch5(net, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553b52c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 30, 57, 57])\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "for xx, yy in data_iter_random(X, Y, 128, 30):\n",
    "    print(xx.size())\n",
    "    print(len(xx))\n",
    "    print(net(xx).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b68712d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cpu\n",
      "torch.Size([128])\n",
      "torch.Size([128, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n",
      "torch.Size([128, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# checkpoint = torch.load('./model_1.pt')\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# net.load_state_dict(checkpoint)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# net.train()\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain_ch5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mtrain_ch5\u001b[0;34m(net, batch_size, optimizer, device, num_epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m l \u001b[38;5;241m=\u001b[39m loss(y_hat, y)\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 21\u001b[0m \u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m train_l_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# checkpoint = torch.load('./model_1.pt')\n",
    "# net.load_state_dict(checkpoint)\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# net.train()\n",
    "train_ch5(net, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f341882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[67.9558],\n",
      "         [67.9596],\n",
      "         [67.9596],\n",
      "         [67.9596],\n",
      "         [67.9596],\n",
      "         [67.9596],\n",
      "         [67.9596],\n",
      "         [67.9596],\n",
      "         [67.9596],\n",
      "         [67.9596],\n",
      "         [67.9596],\n",
      "         [67.9596],\n",
      "         [67.9596]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_test = X[0:30, :, :, :]\n",
    "x_test = x_test.transpose(0, 1)\n",
    "x_test = x_test.view(1, 3, 30, 57, 57)\n",
    "print(net(x_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
